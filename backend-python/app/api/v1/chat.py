"""
ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
é€šå¸¸ãƒãƒ£ãƒƒãƒˆã¨ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆ
"""
from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from app.models.request import ChatRequest
from app.models.response import ChatResponse
from app.services.agent_service import AgentService
from app.services.registry import get_registry
from app.core.streaming import SSEStreamingCallback
from app.core.llm_factory import LLMFactory
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
import json
import asyncio
import time
import logging

router = APIRouter()
logger = logging.getLogger(__name__)


@router.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    é€šå¸¸ãƒãƒ£ãƒƒãƒˆ
    
    Args:
        request: ãƒãƒ£ãƒƒãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        
    Returns:
        ChatResponse: ãƒãƒ£ãƒƒãƒˆçµæœ
    """
    logger.info(f"Chat request from user {request.user_id}")
    
    # ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰ãƒ„ãƒ¼ãƒ«ã‚’å–å¾—
    tools = []
    registry = get_registry()
    basic_tools_count = 0
    service_tools_count = 0
    
    # ã‚µãƒ¼ãƒ“ã‚¹è¨­å®šã«åŸºã¥ã„ã¦ãƒ„ãƒ¼ãƒ«ã‚’å–å¾—
    if request.services:
        for service_config in request.services:
            service_class = registry.get_service_class(service_config.service_class)
            if service_class:
                # ã‚µãƒ¼ãƒ“ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆï¼ˆèªè¨¼æƒ…å ±ä»˜ãï¼‰
                auth = {}
                if service_config.api_key:
                    auth["api_key"] = service_config.api_key
                
                service = service_class(
                    config=service_config.headers or {},
                    auth=auth
                )
                service_tools = service.get_langchain_tools()
                
                # ãƒ„ãƒ¼ãƒ«é¸æŠãƒ¢ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if service_config.tool_selection_mode == "selected" and service_config.selected_tools:
                    service_tools = [
                        tool for tool in service_tools
                        if tool.name in service_config.selected_tools
                    ]
                
                tools.extend(service_tools)
                
                # ã‚µãƒ¼ãƒ“ã‚¹ã‚¿ã‚¤ãƒ—ã”ã¨ã«ã‚«ã‚¦ãƒ³ãƒˆ
                if service.SERVICE_TYPE == 'built_in':
                    basic_tools_count += len(service_tools)
                else:
                    service_tools_count += len(service_tools)
                
                logger.info(f"Loaded {len(service_tools)} tools from {service_config.service_class}")
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚µãƒ¼ãƒ“ã‚¹ã§ãƒãƒ£ãƒƒãƒˆå®Ÿè¡Œ
    agent_service = AgentService()
    response = await agent_service.execute_chat(request, tools)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ãƒ„ãƒ¼ãƒ«æ•°ã‚’è¨­å®š
    response.metadata.basic_tools_count = basic_tools_count
    response.metadata.service_tools_count = service_tools_count
    
    return response


@router.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """
    ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆ
    
    Args:
        request: ãƒãƒ£ãƒƒãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        
    Returns:
        StreamingResponse: SSEã‚¹ãƒˆãƒªãƒ¼ãƒ 
    """
    logger.info(f"Streaming chat request from user {request.user_id}")
    
    async def event_generator():
        try:
            # ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰ãƒ„ãƒ¼ãƒ«ã‚’å–å¾—
            tools = []
            registry = get_registry()
            
            logger.info(f"ğŸ“¦ Services in request: {len(request.services) if request.services else 0}")
            logger.info(f"ğŸ“¦ Service details: {[s.service_class for s in request.services] if request.services else []}")
            
            # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: ã‚µãƒ¼ãƒ“ã‚¹è¨­å®šã®è©³ç´°ã‚’ç¢ºèª
            if request.services:
                for i, service_config in enumerate(request.services):
                    logger.info(f"ğŸ“¦ Service {i}: class={service_config.service_class}, api_key={'***' if service_config.api_key else 'None'}, headers={service_config.headers}")
            
            # ã‚µãƒ¼ãƒ“ã‚¹è¨­å®šã«åŸºã¥ã„ã¦ãƒ„ãƒ¼ãƒ«ã‚’å–å¾—
            if request.services:
                for service_config in request.services:
                    service_class = registry.get_service_class(service_config.service_class)
                    if service_class:
                        # ã‚µãƒ¼ãƒ“ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆï¼ˆèªè¨¼æƒ…å ±ä»˜ãï¼‰
                        auth = {}
                        if service_config.api_key:
                            auth["api_key"] = service_config.api_key
                        
                        service = service_class(
                            config=service_config.headers or {},
                            auth=auth
                        )
                        service_tools = service.get_langchain_tools()
                        
                        # ãƒ„ãƒ¼ãƒ«é¸æŠãƒ¢ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                        if service_config.tool_selection_mode == "selected" and service_config.selected_tools:
                            service_tools = [
                                tool for tool in service_tools
                                if tool.name in service_config.selected_tools
                            ]
                        
                        tools.extend(service_tools)
                        logger.info(f"Loaded {len(service_tools)} tools from {service_config.service_class}")
            
            # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½œæˆï¼ˆãƒ„ãƒ¼ãƒ«èª­ã¿è¾¼ã¿å¾Œã«ä½œæˆï¼‰
            callback = SSEStreamingCallback()

            # LLMä½œæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šï¼‰
            llm = LLMFactory.create_llm(
                provider=request.agent_config.provider,
                model=request.agent_config.model,
                temperature=request.agent_config.temperature,
                max_tokens=request.agent_config.max_tokens,
                streaming=True,
                callbacks=[callback]
            )
            
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            system_prompt = AgentService._build_system_prompt(
                request.agent_config.persona,
                request.agent_config.custom_system_prompt
            )
            
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                MessagesPlaceholder(variable_name="chat_history", optional=True),
                ("human", "{input}"),
                MessagesPlaceholder(variable_name="agent_scratchpad"),
            ])
            
            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
            agent = create_openai_tools_agent(llm, tools, prompt)
            agent_executor = AgentExecutor(
                agent=agent,
                tools=tools,
                max_iterations=10,
                max_execution_time=120,
                return_intermediate_steps=True,
                handle_parsing_errors=True,
                callbacks=[callback],
                verbose=True  # ãƒ‡ãƒãƒƒã‚°ã®ãŸã‚verboseã‚’æœ‰åŠ¹åŒ–
            )
            
            logger.info(f"ğŸ¤– Agent executor created with {len(tools)} tools and callback registered")
            
            # ä¼šè©±å±¥æ­´å¤‰æ›
            chat_history = AgentService._convert_history_to_messages(request.conversation_history)

            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œã¨ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ä¸¦è¡Œå‡¦ç†
            start_time = time.time()
            
            async def run_agent():
                """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œï¼ˆastream_eventsã§ãƒ„ãƒ¼ãƒ«ã‚¤ãƒ™ãƒ³ãƒˆã®ã¿ã‚­ãƒ£ãƒ—ãƒãƒ£ï¼‰"""
                try:
                    # astream_eventsã‚’ä½¿ç”¨ã—ã¦ãƒ„ãƒ¼ãƒ«ã‚¤ãƒ™ãƒ³ãƒˆã®ã¿ã‚­ãƒ£ãƒ—ãƒãƒ£
                    # ãƒˆãƒ¼ã‚¯ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã¯SSEStreamingCallbackã®on_llm_new_tokenã«ä»»ã›ã‚‹
                    async for event in agent_executor.astream_events(
                        {
                            "input": request.message,
                            "chat_history": chat_history
                        },
                        version="v2",
                        config={"callbacks": [callback]}
                    ):
                        kind = event["event"]
                        
                        # ãƒ„ãƒ¼ãƒ«é–‹å§‹ã‚¤ãƒ™ãƒ³ãƒˆ
                        if kind == "on_tool_start":
                            tool_name = event.get("name", "Unknown")
                            tool_input = event["data"].get("input", {})
                            
                            # ç¾åœ¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½ç½®ã‚’è¨˜éŒ²
                            insert_position = len("".join(callback.accumulated_tokens))
                            callback.tool_insert_positions[tool_name] = insert_position
                            callback.tool_start_times[tool_name] = time.time()
                            
                            logger.info(f"ğŸ”§ Tool start captured via astream_events: {tool_name}")
                            
                            await callback.queue.put({
                                "type": "tool_start",
                                "tool_id": tool_name,
                                "tool_name": tool_name,
                                "input": str(tool_input),
                                "insert_position": insert_position
                            })
                        
                        # ãƒ„ãƒ¼ãƒ«çµ‚äº†ã‚¤ãƒ™ãƒ³ãƒˆ
                        elif kind == "on_tool_end":
                            tool_name = event.get("name", "Unknown")
                            tool_output = event["data"].get("output", "")
                            
                            # å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
                            execution_time_ms = 0
                            if tool_name in callback.tool_start_times:
                                execution_time_ms = int((time.time() - callback.tool_start_times[tool_name]) * 1000)
                                del callback.tool_start_times[tool_name]
                            
                            # æŒ¿å…¥ä½ç½®ã‚’å–å¾—
                            insert_position = callback.tool_insert_positions.get(tool_name, 0)
                            if tool_name in callback.tool_insert_positions:
                                del callback.tool_insert_positions[tool_name]
                            
                            logger.info(f"âœ… Tool end captured via astream_events: {tool_name}")
                            
                            # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—æƒ…å ±ã‚’è“„ç©
                            tool_call_info = {
                                "tool_id": tool_name,
                                "tool_name": tool_name,
                                "status": "completed",
                                "input": event["data"].get("input", {}),
                                "output": str(tool_output)[:500],
                                "error": None,
                                "execution_time_ms": execution_time_ms,
                                "insert_position": insert_position
                            }
                            callback.tool_calls.append(tool_call_info)
                            
                            await callback.queue.put({
                                "type": "tool_end",
                                "tool_id": tool_name,
                                "status": "completed",
                                "output": str(tool_output)[:500],
                                "error": None,
                                "execution_time_ms": execution_time_ms
                            })
                        
                        # LLMã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¤ãƒ™ãƒ³ãƒˆã¯å‡¦ç†ã—ãªã„ï¼ˆSSEStreamingCallbackã®on_llm_new_tokenã§å‡¦ç†ã•ã‚Œã‚‹ï¼‰
                    
                    # å‡¦ç†å®Œäº†æ™‚ã«å®Œå…¨ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€doneã‚¤ãƒ™ãƒ³ãƒˆã‚’é€ä¿¡
                    processing_time_ms = int((time.time() - start_time) * 1000)
                    
                    basic_tools_count = 0
                    service_tools_count = 0
                    
                    # ã‚µãƒ¼ãƒ“ã‚¹ã‚¿ã‚¤ãƒ—ã”ã¨ã«ãƒ„ãƒ¼ãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                    for service_config in request.services:
                        service_class = registry.get_service_class(service_config.service_class)
                        if service_class:
                            service = service_class()
                            if service.SERVICE_TYPE == 'built_in':
                                basic_tools_count += len([t for t in callback.tool_calls if t['tool_name'] in [tool.name for tool in service.get_langchain_tools()]])
                            else:
                                service_tools_count += len([t for t in callback.tool_calls if t['tool_name'] in [tool.name for tool in service.get_langchain_tools()]])
                    
                    # ç”Ÿæˆã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                    completion_text = "".join(callback.accumulated_tokens)
                    
                    # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’å–å¾—ï¼ˆcallback.token_usageã‹ã‚‰ï¼‰
                    tokens_used = callback.token_usage
                    logger.info(f"ğŸ’° Token usage from API: {tokens_used}")
                    
                    await callback.queue.put({
                        "type": "done",
                        "conversation_id": request.conversation_id,
                        "message": completion_text,
                        "tool_calls": callback.tool_calls,
                        "metadata": {
                            "model": request.agent_config.model,
                            "provider": request.agent_config.provider,
                            "tokens_used": tokens_used,
                            "processing_time_ms": processing_time_ms,
                            "completion_mode_used": "streaming",
                            "tools_available": len(tools),
                            "basic_tools_count": basic_tools_count,
                            "service_tools_count": service_tools_count
                        }
                    })
                except Exception as e:
                    logger.error(f"Agent execution error: {e}", exc_info=True)
                    await callback.queue.put({
                        "type": "error",
                        "code": "AGENT_ERROR",
                        "message": str(e)
                    })

            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§é–‹å§‹
            agent_task = asyncio.create_task(run_agent())

            # ã‚¤ãƒ™ãƒ³ãƒˆã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒ 
            try:
                async for event in callback.get_events():
                    yield f"data: {json.dumps(event, ensure_ascii=False)}\n\n"
            finally:
                # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡ŒãŒå®Œäº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ
                if not agent_task.done():
                    await agent_task
        
        except Exception as e:
            logger.error(f"Streaming error: {e}", exc_info=True)
            yield f"data: {json.dumps({'type': 'error', 'code': 'INTERNAL_ERROR', 'message': str(e)}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive"
        }
    )

