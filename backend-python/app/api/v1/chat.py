"""
ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
é€šå¸¸ãƒãƒ£ãƒƒãƒˆã¨ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆ
"""
from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from app.models.request import ChatRequest
from app.models.response import ChatResponse
from app.services.agent_service import AgentService
from app.services.mcp_service import MCPService
from app.tools.basic_tools import get_basic_tools
from app.core.streaming import SSEStreamingCallback
from app.core.llm_factory import LLMFactory
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
import json
import asyncio
import time
import logging

router = APIRouter()
logger = logging.getLogger(__name__)


@router.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    é€šå¸¸ãƒãƒ£ãƒƒãƒˆ
    
    Args:
        request: ãƒãƒ£ãƒƒãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        
    Returns:
        ChatResponse: ãƒãƒ£ãƒƒãƒˆçµæœ
    """
    logger.info(f"Chat request from user {request.user_id}")
    
    # ãƒ„ãƒ¼ãƒ«ã‚’å–å¾—
    tools = []
    
    # åŸºæœ¬ãƒ„ãƒ¼ãƒ«ã‚’è¿½åŠ 
    if request.include_basic_tools:
        basic_tools = get_basic_tools()
        tools.extend(basic_tools)
        logger.info(f"Added {len(basic_tools)} basic tools")
    
    # MCPãƒ„ãƒ¼ãƒ«ã‚’è¿½åŠ 
    mcp_tools = await MCPService.load_all_tools(request.mcp_servers)
    tools.extend(mcp_tools)
    logger.info(f"Added {len(mcp_tools)} MCP tools")
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚µãƒ¼ãƒ“ã‚¹ã§ãƒãƒ£ãƒƒãƒˆå®Ÿè¡Œ
    agent_service = AgentService()
    response = await agent_service.execute_chat(request, tools)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ãƒ„ãƒ¼ãƒ«æ•°ã‚’è¨­å®š
    response.metadata.basic_tools_count = len(get_basic_tools()) if request.include_basic_tools else 0
    response.metadata.mcp_tools_count = len(mcp_tools)
    
    return response


@router.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """
    ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆ
    
    Args:
        request: ãƒãƒ£ãƒƒãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        
    Returns:
        StreamingResponse: SSEã‚¹ãƒˆãƒªãƒ¼ãƒ 
    """
    logger.info(f"Streaming chat request from user {request.user_id}")
    
    async def event_generator():
        try:
            # ãƒ„ãƒ¼ãƒ«ã‚’å–å¾—
            tools = []
            
            # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½œæˆï¼ˆãƒ„ãƒ¼ãƒ«èª­ã¿è¾¼ã¿å‰ã«ä½œæˆï¼‰
            callback = SSEStreamingCallback()
            
            if request.include_basic_tools:
                basic_tools = get_basic_tools()
                # å„ãƒ„ãƒ¼ãƒ«ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¨­å®š
                for tool in basic_tools:
                    tool.callbacks = [callback]
                tools.extend(basic_tools)
                logger.info(f"âœ… Loaded {len(basic_tools)} basic tools with callbacks")
                for tool in basic_tools:
                    logger.info(f"  ğŸ“¦ Tool: {tool.name}")
            
            mcp_tools = await MCPService.load_all_tools(request.mcp_servers)
            # MCPãƒ„ãƒ¼ãƒ«ã«ã‚‚ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¨­å®š
            for tool in mcp_tools:
                tool.callbacks = [callback]
            tools.extend(mcp_tools)
            logger.info(f"âœ… Total tools available: {len(tools)} (all with callbacks)")

            # LLMä½œæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šï¼‰
            llm = LLMFactory.create_llm(
                provider=request.agent_config.provider,
                model=request.agent_config.model,
                temperature=request.agent_config.temperature,
                max_tokens=request.agent_config.max_tokens,
                streaming=True,
                callbacks=[callback]
            )
            
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            system_prompt = AgentService._build_system_prompt(
                request.agent_config.persona,
                request.agent_config.custom_system_prompt
            )
            
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                MessagesPlaceholder(variable_name="chat_history", optional=True),
                ("human", "{input}"),
                MessagesPlaceholder(variable_name="agent_scratchpad"),
            ])
            
            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
            agent = create_openai_tools_agent(llm, tools, prompt)
            agent_executor = AgentExecutor(
                agent=agent,
                tools=tools,
                max_iterations=10,
                max_execution_time=120,
                return_intermediate_steps=True,
                handle_parsing_errors=True,
                callbacks=[callback],
                verbose=True  # ãƒ‡ãƒãƒƒã‚°ã®ãŸã‚verboseã‚’æœ‰åŠ¹åŒ–
            )
            
            logger.info(f"ğŸ¤– Agent executor created with {len(tools)} tools and callback registered")
            
            # ä¼šè©±å±¥æ­´å¤‰æ›
            chat_history = AgentService._convert_history_to_messages(request.conversation_history)

            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œã¨ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ä¸¦è¡Œå‡¦ç†
            start_time = time.time()
            
            async def run_agent():
                """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œ"""
                try:
                    await agent_executor.ainvoke({
                        "input": request.message,
                        "chat_history": chat_history
                    })
                    
                    # å‡¦ç†å®Œäº†æ™‚ã«å®Œå…¨ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€doneã‚¤ãƒ™ãƒ³ãƒˆã‚’é€ä¿¡
                    processing_time_ms = int((time.time() - start_time) * 1000)
                    
                    basic_tools_count = len(basic_tools) if request.include_basic_tools else 0
                    
                    # ç”Ÿæˆã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                    completion_text = "".join(callback.accumulated_tokens)
                    
                    # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’å–å¾—ï¼ˆcallback.token_usageã‹ã‚‰ï¼‰
                    # stream_usage=Trueã§è¨­å®šã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€on_llm_endã§å–å¾—ã§ãã‚‹
                    tokens_used = callback.token_usage
                    logger.info(f"ğŸ’° Token usage from API: {tokens_used}")
                    
                    await callback.queue.put({
                        "type": "done",
                        "conversation_id": request.conversation_id,
                        "message": completion_text,
                        "tool_calls": callback.tool_calls,
                        "metadata": {
                            "model": request.agent_config.model,
                            "provider": request.agent_config.provider,
                            "tokens_used": tokens_used,
                            "processing_time_ms": processing_time_ms,
                            "completion_mode_used": "streaming",
                            "tools_available": len(tools),
                            "basic_tools_count": basic_tools_count,
                            "mcp_tools_count": len(mcp_tools)
                        }
                    })
                except Exception as e:
                    logger.error(f"Agent execution error: {e}", exc_info=True)
                    await callback.queue.put({
                        "type": "error",
                        "code": "AGENT_ERROR",
                        "message": str(e)
                    })

            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§é–‹å§‹
            agent_task = asyncio.create_task(run_agent())

            # ã‚¤ãƒ™ãƒ³ãƒˆã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒ 
            try:
                async for event in callback.get_events():
                    yield f"data: {json.dumps(event, ensure_ascii=False)}\n\n"
            finally:
                # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡ŒãŒå®Œäº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ
                if not agent_task.done():
                    await agent_task
        
        except Exception as e:
            logger.error(f"Streaming error: {e}", exc_info=True)
            yield f"data: {json.dumps({'type': 'error', 'code': 'INTERNAL_ERROR', 'message': str(e)}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive"
        }
    )

